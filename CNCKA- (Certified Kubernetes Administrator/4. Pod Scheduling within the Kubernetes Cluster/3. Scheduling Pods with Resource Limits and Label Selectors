Scheduling Pods with Resource Limits and Label Selectors:
=========================================================

In order to share the resources of your node properly, you can set resource limits and requests in Kubernetes. This allows you to reserve enough CPU and memory for your application while still maintaining system health. we will create some requests and limits in our pod YAML to show how it’s used by the node.

View the capacity and the allocatable info from a node:
--------------------------------------------------------
#kubectl describe nodes

The pod YAML for a pod with requests:
--------------------------------------
vi resource-pod1.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod1
    resources:
      requests:
        cpu: 800m
        memory: 20Mi
		
Create the requests pod:
------------------------
#kubectl create -f resource-pod1.yaml

View the pods and nodes they landed on:
----------------------------------------
#kubectl get pods -o wide

The YAML for a pod that has a large request:
---------------------------------------------
vi resource-pod2.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod2
    resources:
      requests:
        cpu: 1000m
        memory: 20Mi
		
Create the pod with 1000 millicore request:
--------------------------------------------
#kubectl create -f resource-pod2.yaml

See why the pod with a large request didn’t get scheduled:
-----------------------------------------------------------
#kubectl describe resource-pod2

Look at the total requests per node:
-------------------------------------
#kubectl describe nodes chadcrowell3c.mylabserver.com

Delete the first pod to make room for the pod with a large request:
-------------------------------------------------------------------
#kubectl delete pods resource-pod1

Watch as the first pod is terminated and the second pod is started:
-------------------------------------------------------------------
#kubectl get pods -o wide -w

The YAML for a pod that has limits:
-----------------------------------
vi limited-pod.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
		
Create a pod with limits:
--------------------------
#kubectl create -f limited-pod.yaml

Use the exec utility to use the top command:
---------------------------------------------
#kubectl exec -it limited-pod top